= S3 Integration


:product-name: Globus Transfer

////
Todos can come here
////


== Overview

{product-name} supports using an Amazon S3 bucket as an endpoint to store
files.  The bucket can be in any region.  

{product-name} supports the following features on S3 endpoints:

* You may transfer files to and from S3 endpoints, as long as
the other side is an updated GridFTP server.  Transfers from an S3 endpoint to
another S3 endpoint are not supported.

* You may create ACLs on the endpoint to give other GO users
access to directories.   These ACLs are created and stored in {product-name},
not S3.

* Interactive directory listings

* Interactive directory creation (mkdir)

* Deleting files and directories

NOTE: A Managed Endpoint subscription is required in order to create S3
endpoints.  Please contact support@globus.org for more information. 

NOTE: Amazon S3 is an eventually-consistent system by design and {product-name}
can not guarantee stronger levels of consistency.




== Installing GridFTP

You need to build and/or install the latest GT beta.  That has the Verisign CA
automatically bundled (in source code) and is used if GO does not override it.
There is no CA cert management required by the endpoint admin to use S3
buckets, either in http:// or https:// mode.

The following GridFTP installation types are supported:

* Regular
* Globus Shared Endpoint
* Globus Connect Personal
* Globus Connect Server


== Creating an Endpoint

----
$ endpoint-add --s3 mys3
Enter the full URL of your S3 bucket.
You must use the region specific endpoint.
Examples:
  US Standard:    https://s3.amazonaws.com/my.bucket.name
  US West Oregon: https://s3-us-west-2.amazonaws.com/my.bucket.name
  EU Ireland:     https://s3-eu-west-1.amazonaws.com/my.bucket.name
> https://s3.amazonaws.com/my.bucket

Select Activation Option
 1  Endpoint owner (you) activates the endpoint for all users
 2  Each user activates the endpoint themselves
> 1
Added the endpoint 'karlito#mys3'
Activate it by running: endpoint-activate --s3 karlito#mys3
----

Both http:// and https:// endpoints are supported.  HTTP endpoints do not
support data channel encryption, but might be faster.

NOTE: The S3 bucket must not be a ‘requester pays for bandwidth’ bucket.  If it
is, all operations will fail, because transfer will not indicate via HTTP
headers that it is willing to pay for bandwidth charges.


== Activating an Endpoint

There are currently two ways to activate (set security credentials) the S3
endpoint.  They are described below.

NOTE: Activation currently only works through the CLI, but if the endpoint is
‘owner activated’ other users can use it in the GUI.

NOTE: Currently, anonymous activation is not supported.


=== Using a IAM Role

The most secure way to allow access is to create a IAM Role and have
{product-name} assume it.  This restricts access to only the {product-name} AWS
account.

----
$ endpoint-activate --s3 mys3
Select Activation Type:
 1  AWS Key and Secret
 2  Assume Role
> 2
You have selected the Assume Role option.
You should have created an AWS Role with these properties:
- Trusted Entity is the Globus Online AWS Account '328067584297'
- External ID is 'user:karlito'
- Policy grants access to S3 buckets and/or objects

Enter the role ARN (ex: arn:aws:iam::123456789000:role/myrole)
> arn:aws:iam::052825605068:role/role-for-go-prod-test
Activating 'karlito#mys3'
----

The External ID contains the username of the GO user doing the activation (i.e.
the owner of the credential).  For owner activated endpoints, that’s the
endpoint owner.  Otherwise, it’s each user of the endpoint.


NOTE: For security, the External ID is different on the test and QA
environments.  It will have an additional prefix of "test-", "qa"-, etc.


=== Using a IAM User Credential

It is _highly recommended_ to use a IAM user credential, and not an account
credential, because the IAM user can be constrained to a minimum set of
permissions (see <<sample_policy,Sample IAM Policy>>).

Run the +endpoint-activate --s3+ command on the endpoint.  
Then, input the following:

* AWS Key ID
* AWS Secret Key
* AWS STS Token (Required for IAM credentials)
* The hours remaining until credential expiration

NOTE:  {product-name} can not automatically detect the credential expiration
time.  If you provide that information, GO can send out alert emails prior to
credential expiration.



== Sharing With Other Users

----
$ acl-add mys3 --user karl2 --perm rw --path /foo/
$ acl-list mys3
----

This adds an access rule giving +karl2+ +read-write+ access to the directory
+/foo/+ on the S3 endpoint +mys3+, and then displays all of the endpoint's
access rules.

Group ACLs are not supported in the CLI, but may be added via the REST API or
web site.

For more information, see <<acl,Access Control List>>.


== Listing Directories

----
$ ls -l karlito#mys3/foo/
----

This lists files and "sub directories" inside the given directory.

Not Supported: 

* Listing a file (the path must end with /)
* Globbing


== Creating Directories

----
$ mkdir karlito#mys3/bar/
----

This creates a directory marker (an empty file) called +/bar/+ in S3.   Note
that parent directory markers are not required to exist.  However, the command
will fail if the directory marker already exists or there is a file with the
same base name, e.g. +/bar+.


== Transferring Files

----
$ transfer -- mys3/file.txt go#ep1/~/myfile  # Once we upgrade go#ep1
$ transfer -- go#ep1/~/myfile mys3/upload.txt   # Once we upgrade go#ep1
----


=== Supported Transfer Options

The following transfer options are supported:

* Recursive Directory Transfer

** _When uploading to S3_: All files in a directory structure (except for
symlinks, per normal transfer behavior) are uploaded to S3.  
+
CAUTION: Directory markers, and in particular empty directories, are not explicitly
created in S3.

** _When downloading from S3_: All objects are downloaded, except for objects
whose path name ends with a slash (+'/'+).  The latter is assumed to be a
directory marker and will be created as a directory on the GridFTP endpoint,
not a file.


=== Unsupported Transfer Options

These options are not supported and _will be ignored_ if set.

* Verify-Checksum
* Verify-Size
* Staging from tape (for MSS endpoints)
* --perf-cc, --perf-p, --perf-pp

** Transfer currently uses a single data stream per task.  A future release may
increase the number of parallel streams, which could improve performance for
lots of small files.


These options are not supported and will raise an error, causing the task
submission to fail.

* Sync
* Sync-Delete
* Preserve-Modification-Time
* SCP-to-S3 (S3 does not have directories)
* S3-to-S3 transfer


=== Additional Notes

WARNING: Transfer will continually retry errors.  This will incur additional S3
API and bandwidth costs.

WARNING: Incomplete uploads will not be removed.  These will incur additional
S3 storage costs.   Ideally, Amazon would add support for auto expiration.

NOTE: All files are uploaded to S3 using the multi-part API; thus they will not
have verifiable md5 checksums (the etag will not be an md5 hash).  Ideally,
Amazon would fix this.

NOTE: Server-side-encryption of ‘AES-256’ is automatically requested for all
uploads.


=== Informational Events

When a S3 transfer job is running, +PROGRESS+ and perhaps +SUCCEEDED+ events
will be generated every 60 seconds.   In addition, the full list of
successfully transferred files can be requested via the rest-api ‘successful
xfers’ resource or CLI details -t. 

NOTE: The format and frequency of event messages is subject to change.
Currently, events are primarily intended for human monitoring and
troubleshooting.

.Transfer Events Example

----
Time          : 2013-12-10 21:36:41.557590Z
Code          : PROGRESS
Description   : Performance monitoring event
Details       : {
  "duration": 3.94,
  "mbps": 117.4,
  "bytes_transferred": 57880185
}

Time          : 2013-12-10 23:03:50.233482Z
Code          : SUCCEEDED
Description   : The operation succeeded
Details       : {
  "files_succeeded": 9
}
----


== Deleting Files and Directories

----
$ rm -f karlito#mys3/myfile.txt
$ rm -r -f karlito#mys3/mydirectory/
----

Delete currently requires the force (-f) option, which does not fail if the
target does not exist.  In addition, recursive directory deletes must be
explicitly indicated via a trailing slash (+/+).  

NOTE: If a directory is given without a trailing +/+, {product-name} will
assume it is a single file (object), and no error will be given.

WARNING: Amazon S3 is an eventually-consistent, distributed system.  This means
that for an indeterminate amount of time following a delete or other operation, S3 may
report old and/or varying results.   For example, after a delete operation
finishes you may still see a few files exist, and after an hour it might clear
up.

Globbing is not currently supported.  No progress events are generated for
S3 delete tasks.


== Unsupported Operations

* rename
* S3 only supports utf-8 encoded unicode paths, so servers that send filenames
  improperly (not utf-8), like Windows GCP, will fail when uploading non-ascii
  file names.
* S3 supports non-unix compatible file names such as ‘.’, ‘..’, and embedded
  ‘//’.


== Technical Notes

=== Credential Security

{product-name} requires an AWS credential to securely sign HTTP/HTTPS upload
and download requests to S3.  Each file requres one or more unique requests and
signatures.   The lifetime of a request signature is set by Amazon, but is
quite short - about 5 minutes.  {product-name} generates a signature and sends
a signed request to the GridFTP server, which uses it to directly upload or
download the file 

Note that the file's data is not proxied through {product-name}.  The
AWS credential is never sent to the GridFTP server.



[[sample_policy]]
=== Sample IAM Policy

This IAM policy allows {product-name} read and write access to a single S3
bucket.  The first statement allows directory listings (ListBucket).  The
second statement allows read (GetObject) and write (PutObject) access to
file objects.

NOTE: To allow normal directory browsing while using {product-name} ACLs, the
policy should allow +ListBucket+ on the entire bucket, as in this example.  

This policy purposely does not allow all possible S3 operations, such as
DeleteObject or DeleteBucket - see the Amazon S3 Developer Guide for details.

.Sample Amazon IAM Policy
----
{
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:ListBucket",
      "Resource": "arn:aws:s3:::my.test.bucket"
    },
    {
      "Effect": "Allow",
      "Action": ["s3:GetObject", "s3:PutObject"],
      "Resource": "arn:aws:s3:::my.test.bucket/*" 
    }
  ]
}
----


[[file_and_directory_semantics]]
=== File and Directory Semantics

S3 does not have directories in the traditional sense; it only has a keyspace
and an API to list keys based on a prefix and delimiter.  {product-name}, like
the Amazon S3 Web Console, uses +/+ as a delimiter to mimic standard filesystem
semantics.

1. All paths submitted to {product-name} must start with +/+.  The S3 keys do
not technically start with +/+, but {product-name} handles this mapping
automatically.  

2. All paths submitted to {product-name} must be normalized; they must not
contain path segments of +/../+, +/./+, or +//+. 

3. A *directory path* ends with +/+.

4. A *file path* does not end with +/+.

5. A key name that ends with +/+ is considered a *directory marker*.  These can
also be created in the Amazon S3 Console (i.e. "create folder").  The contents
of a directory marker object is ignored; it will be created as a directory when
transferring to a GridFTP endpoint.   

6. The *contents* of a directory is the list of all keys that begin with the
directory path, with any text after the next +/+ removed, and duplicates
removed.  This is the output of the 
+ListBucket(prefix=directory-path/, delimiter=/)+ S3 API.
+ 
Anything in Contents other then a directory marker is considered a file.
Anything in CommonPrefixes is considered a directory.  

7. For interactive dirlistings, if a directory marker of +directory-path/+
exists, it is rewritten to +.+ for display and displayed only if the user
requests hidden files.  

8. An *empty directory* is a directory that only contains the directory marker.
   
9. A *non-existent* directory contains no keys at all, including the directory
marker.  {product-name} will return a +Not Found+ error if a non existent
directory is requested.


[[acl]]
=== Access Control List (ACL)

The owner of an S3 endpoint can share directories with other {product-name}
users by adding rules to the endpoint's access control list (ACL).  

A newly created endpoint has an empty ACL, meaning no other {product-name}
users initially have access.  The endpoint's owner always has full read-write
access.  

NOTE: Only owner-activated S3 endpoints can have an ACL.

The owner's AWS credential is used by {product-name} for all access to S3, but
{product-name} further restricts access based on the access rules that apply to
each requesting {product-name} user.  


Each rule in the access control list has the following:

principal::
    Who the rule applies to.  The principal must be a {product-name}
    user name or group UUID.  

directory path::
    What directory the rule applies to (recursively).  A valid directory path
as described in section 
<<file_and_directory_semantics,File and Directory Semantics>>.   

permission::
    The permissions granted by the rule: either +read-only+ or +read-write+.
+read-only+ allows directory listings and file download; +read-write+ also
allows file upload and overwrite.

CAUTION: Access rules are always additive, so permissions cannot be further restricted
in subdirectories.  For example, if a rule gives the user read-write access to
+/projects/+ and another rule gives read-only access to +/projects/study1/+,
then the user is still granted full read-write access to +/projects/study1/+.

==== Directories

An access rule on a directory path provides implicit rights to browse to that
directory from the root of the bucket, but all files and directories in parent
directories without an associated access rule will be hidden. 

In detail:

1. If you have an access rule allowing read access to a directory, you may list
everything in that directory and any of its sub directories.

2. If you don't have read access to a directory but you do have read access to
a subdirectory, you may still list the top level directory but {product-name}
will return a partial listing and filter out files and subdirectories for which
you have no access.  For example, if you only have an access rule allowing
+/projects/foo/+, a directory listing of +/+ will only show +/projects/+, and
not +/admin/+ or +/root.txt+.  

3. If you attempt to list a path for which you have no access, for example
+/admin/+ or +/projects/bar/+, {product-name} will return a  
+Permission Denied+ error.

When recursively transferring a directory from S3, only files and directories
visible to the user according to these rules will be copied; the transfer will
proceed as if only those files and directories exist.

==== Files

If the ACL does not allow a file specified in a transfer request,
{product-name} will generate a +Permission Denied+ error.


==== AWS Permissions

To work effectively with {product-name} access rules, the AWS credential used
for activation should allow full +ListBucket+ access on any key prefix.  (See
also <<sample_policy,Sample IAM Policy>>)

If this is not the case, an error will be displayed when trying to
interactively list a directory path that is not allowed for ListBucket, and
users will be required to explicitly type a directory path that is allowed.  A
recursive transfer download of a directory path not allowed by ListBucket will
also fail.

== Change History

==== 3.6

* Add interactive mkdir
* Add rm/delete

==== 3.5

* Allow partial directory listings for parent directories of ACL rules

* Recursive download of non-existent s3 directory will generate a +Not Found+
error.

* Error code specified as +Permission Denied+ for operations denied by ACL

* Require normalized paths for S3 interaction

* Require normalized paths in ACL rules

* ACLs require an owner-activated S3 endpoint

* Clarified that ACL rules are additive-permission only
